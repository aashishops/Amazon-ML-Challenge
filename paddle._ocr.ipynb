{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import signal\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR\n",
    "from ppocr.utils.logging import get_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = r'student_resource3\\dataset\\train.csv'\n",
    "\n",
    "# Output folder for the OCR processed images and CSV\n",
    "output_folder = r'paddle_ocr_output'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV (limit to 100 rows for this example)\n",
    "df = pd.read_csv(csv_file_path).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_row = 200\n",
    "\n",
    "if start_row >= len(df):\n",
    "    raise ValueError(\"Starting row index exceeds the number of rows in the DataFrame\")\n",
    "df = df.iloc[start_row:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "chunk_size = 50  # Save the CSV after every 5 rows\n",
    "chunk_counter = 0\n",
    "initial_chunk_number = start_row // chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    img_url = row['image_link']\n",
    "    group_id = row['group_id']\n",
    "    entity_name = row['entity_name']\n",
    "    entity_value = row['entity_value']\n",
    "\n",
    "    ocr_result_text = \"\"\n",
    "\n",
    "    try:\n",
    "        # Download the image from the URL\n",
    "        response = requests.get(img_url)\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "        # Convert the image to a NumPy array\n",
    "        img_np = np.array(img)\n",
    "\n",
    "        # Perform OCR on the image\n",
    "        result = ocr.ocr(img_np, cls=True)\n",
    "\n",
    "        if result is not None:\n",
    "            # Concatenate all detected texts with \";\" delimiter\n",
    "            ocr_result_text = \"; \".join([line[1][0] for res in result for line in res])\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error and skip to the next row\n",
    "        logger.error(f\"Error processing image {img_url}: {e}\")\n",
    "        return None  # Return None to indicate an error occurred\n",
    "\n",
    "    # Return the processed row as a dictionary\n",
    "    return {\n",
    "        'group_id': group_id,\n",
    "        'text': ocr_result_text.strip(),  # Clean up trailing spaces\n",
    "        'entity_name': entity_name,\n",
    "        'entity_value': entity_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_next_chunk_filename(chunk_number):\n",
    "    \"\"\"Generate a filename for the next chunk, checking if it already exists.\"\"\"\n",
    "    while True:\n",
    "        output_csv_path = os.path.join(output_folder, f'ocr_results_chunk_{chunk_number}.csv')\n",
    "        if not os.path.exists(output_csv_path):\n",
    "            return output_csv_path\n",
    "        chunk_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunk(processed_data, chunk_number):\n",
    "    if processed_data:\n",
    "        output_csv_path = get_next_chunk_filename(chunk_number)\n",
    "        pd.DataFrame(processed_data).to_csv(output_csv_path, index=False)\n",
    "        print(f\"Chunk saved to {output_csv_path}\")\n",
    "        processed_data.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt detected! Saving current progress...\n",
      "OCR processing complete. All data saved.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    chunk_counter = initial_chunk_number\n",
    "    for idx, row in df.iterrows():\n",
    "        processed_row = process_row(row)\n",
    "        if processed_row is not None:  # Only append if processing was successful\n",
    "            processed_data.append(processed_row)\n",
    "\n",
    "        # Save if we have enough rows\n",
    "        if len(processed_data) >= chunk_size:\n",
    "            save_chunk(processed_data, chunk_counter)\n",
    "            chunk_counter += 1  # Increment chunk counter after saving\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nKeyboardInterrupt detected! Saving current progress...\")\n",
    "\n",
    "finally:\n",
    "    # Save any remaining rows in the buffer if present\n",
    "    if processed_data:\n",
    "        save_chunk(processed_data, chunk_counter)\n",
    "\n",
    "    print(\"OCR processing complete. All data saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files have been merged successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "folder_path = r'paddle_ocr_output'\n",
    "\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df.to_csv('./merged_output.csv', index=False)\n",
    "\n",
    "print(\"All CSV files have been merged successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
