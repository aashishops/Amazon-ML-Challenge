{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aashi\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "from paddleocr import PaddleOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/09/15 22:33:44] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='C:\\\\Users\\\\aashi/.paddleocr/whl\\\\det\\\\en\\\\en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='C:\\\\Users\\\\aashi/.paddleocr/whl\\\\rec\\\\en\\\\en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='c:\\\\Users\\\\aashi\\\\anaconda3\\\\Lib\\\\site-packages\\\\paddleocr\\\\ppocr\\\\utils\\\\en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='C:\\\\Users\\\\aashi/.paddleocr/whl\\\\cls\\\\ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    }
   ],
   "source": [
    "# Initialize PaddleOCR (you only need to do this once)\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en') \n",
    "\n",
    "# Path to your CSV file\n",
    "csv_file_path = r'student_resource3\\dataset\\train.csv'\n",
    "\n",
    "# Output folder for the OCR processed images and CSV\n",
    "output_folder = r'E:\\Career\\hackathon\\Amazon-ML-Challenge\\paddle_ocr_output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Read the CSV (limit to 100 rows for this example)\n",
    "df = pd.read_csv(csv_file_path).head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each row\n",
    "def process_row(row):\n",
    "    img_url = row['image_link']\n",
    "    group_id = row['group_id']\n",
    "    entity_name = row['entity_name']\n",
    "    entity_value = row['entity_value']\n",
    "\n",
    "    ocr_result_text = \"\"\n",
    "\n",
    "    try:\n",
    "        # Download the image from the URL\n",
    "        response = requests.get(img_url)\n",
    "        if response.status_code == 200:\n",
    "            img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "            # Convert the image to a NumPy array\n",
    "            img_np = np.array(img)\n",
    "\n",
    "            # Perform OCR on the image\n",
    "            result = ocr.ocr(img_np, cls=True)\n",
    "\n",
    "            if result is not None:\n",
    "                # Concatenate all detected texts\n",
    "                for res in result:\n",
    "                    if res is not None:\n",
    "                        for line in res:\n",
    "                            text = line[1][0]  # Extract the text only\n",
    "                            ocr_result_text += text + \" \"  # Concatenate the text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {img_url}: {e}\")\n",
    "    \n",
    "    # Return the result as a dictionary for each row\n",
    "    return {\n",
    "        'group_id': group_id,\n",
    "        'text': ocr_result_text.strip(),  # Clean up trailing spaces\n",
    "        'entity_name': entity_name,\n",
    "        'entity_value': entity_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = df.apply(process_row, axis=1)\n",
    "\n",
    "# Break the result into smaller chunks to handle failures\n",
    "chunk_size = 20000  # Adjust based on your preference\n",
    "for i in range(0, len(processed_data), chunk_size):\n",
    "    chunk = processed_data.iloc[i:i+chunk_size]\n",
    "    output_csv_path = os.path.join(output_folder, f'ocr_results_chunk_{i // chunk_size}.csv')\n",
    "    chunk.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Chunk saved to {output_csv_path}\")\n",
    "\n",
    "print(\"OCR processing complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
